var store = [{
        "title": "AWS Portfolio",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/awsportfolio/",
        "teaser": null
      },{
        "title": "Comparing Java LTS Releases",
        "excerpt":"The last 5 years have seen some rapid changes in the way new versions of the Java Development Kit is deployed and maintained.  Traditionally, new Java versions were always released in a 2 year life cycle.  Every 2 years, a new JDK would be released, containing features that emulated (and in some cases worked to replace) other popular languages at the time.  For example, Java 2, released in 1998, saw the addition of the strictfp keyword, which was essential in making the architecture independent Java language competitive with C and the verbosly specific floating point calculations it was used for.  Java 4, released in 2002, added regular expression libraries that behaved similarly to Perl, a language which began to see its usage decline in the 2000’s.  The development of Java has been at a constant trajectory since 2014 with the release of Java 8.  Over the next 4 years, there were 3 new Java releases, effectively doubling the expected number of releases set by precedent.  In 2019, the fifth year since the release of Java 8, Java 12 is slated to be released.  As an application developer, this can be quite overwhelming.  How many new applications do you have to go back and update now?  As a systems engineer, this is a nightmare.  How many servers in your datacenter constantly need updating to the newest Java release?  Why is this happening in the first place?  If there are two things developers hate, one of them is unnecessary change.  I suggest the dramatic changes Java has undergone in the last 5 years have actually saved this language from going the way of COBOL and Fortran before it.   The LTS Concept  It is absolutely crazy to think about keeping several applications or several hundred (thousand?) servers up to date with the newest Java release, especially when 5 have come out in as many years by the end of 2019.  That is why the concept of an LTS was established.  A Java LTS (long-term support) release is a version of Java that will remain the industry standard for several years.  To give you an example of this, Java 8 was released in 2014, it will continue to receive updates until 2020, and extended support will end by 2025.  This is a span of almost 10 years for a stable Java release to be considered an industry standard.  This gives plenty of OS vendors like Microsoft and Red Hat the time to repackage their releases with Java 8, time for application developers to update their applications to take full advantage of Java 8 features, and time for system administrators to update their data centers with Java 8 before a new version is released.  At this time, the only other Java version that is also an LTS version is Java 11, which was released in 2018.  The next release of Java, Java 12, is not slated to be an LTS release, and neither is 13.  If a trend is to be established from LTS releases vs. non-LTS releases, we can extrapolate the following: LTS releases should be upgraded to as they contain summary features which redefine how the language should be used, while non-LTS releases could be upgraded to as they refine some or all of the features introduced in the latest LTS release without impacting the language as a whole.  As you will see later in this article, Java 8 was an LTS release that introduced features that were perfected in Java 9 and Java 10.  Javas 9 and 10 were non-LTS releases that laid the ground work for new features would be introduced in the newest LTS, Java 11.   New Release Features and Trends  Why have there been so many different releases of Java in recent years?  The answer lies in the fast-paced growth of development technologies across the board.  Java was born out of a need for applications to be architecture independent.  As consumer hardware slowly reaches a computing upper-limit, allowing web-based development and virtualization programs to be integral parts to the developer’s workflow, the need to remain hardware agnostic has fallen to the wayside.  IDE’s are being deployed with standard Android or iOS emulators, allowing developers to specify the architecture on which they intend to deploy their final application, regardless of the machine they are developing on.  Web-based IDEs like Cloud9 let you write and compile Java applications over the Internet.  The business need that birthed Java is no longer a requirement for businesses.  In fact, business requirements have changed.   Application programming is a dying paradigm; functional programming is replacing application programming just like the cloud is replacing the data center.  Businesses no longer need a whole application with a CLI, or a GUI.  The technology world is moving to a place where we need to run a function on a stream of data and forward that result to some other application, without caring about where or how the function runs.  This requirement has seen the rise in popularity of Python and R, and the development of completely new languages like Clojure, Groovy, Rust and Swift.  How is a language like Java, which was made relevant because of how it handles hardware, supposed to stay relevant in a world without hardware?  If you take the last 4 releases (and the planned fifth) into consideration, it is easy to see why Java will remain a powerful tool for developers in a functional, serverless world.   Java 8 LTS  The most powerful feature of Java 8 is undoubtedly the lambda expression.  Lambdas are not new to the programming world, they have been used in C-esque languages for many years now.  In fact, they could have been added to Java a long time ago, but the decision to keep them excluded was made on the grounds that lambda expressions have a tendency to be difficult to read.  Nevertheless, lambda expressions were making the lives of Python developers much easier, so they were a shoo-in for the version 8 Java release.  For those that are not familiar with lambda expressions, consider the following snippet of code  for(String i : listOfStrings) {   System.out.println(i); }  With lambda expressions and the corresponding impact they have had on Java libraries and traditional iterative operations, you can condense the above snippet to:  listOfStrings.forEach(i -&gt; System.out.println(i));  At first, this feels like an insignificant win and, in truth, it is.  There is nothing more pretentious than a programmer touting how the turned three lines of code into one.  The real power behind lambda expressions comes with an explanation of functional interfaces, another feature introduced in Java 8.  A functional interface is just like a regular interface, except you define the body of the function specified in said interface anonymously, using lambda expressions.  Consider the following:  interface FormValidator{   boolean validate(String inputField); }  public static void main(String[] args) {   FormValidator isEmail = (s) -&gt; (s.contains(\"@\")) == 0;   FormValidator isPhoneNumber = (s) -&gt; (s.length()) == 9;   FormValidator isGender = (s) -&gt; (s.equalsIgnoreCase(\"M\") || s.equalsIgnoreCase(\"F\")) == 0;    String email = args[0];   String phoneNumber = args[1];   String gender = args[2];    boolean properEmail = isEmail.validate(email);   boolean properPhone = isPhoneNumber.validate(phoneNumber);   boolean properGender = isGender.validate(gender);    if(properEmail &amp;&amp; properPhone &amp;&amp; properGender){     System.out.println(\"Form Validated.\");   } }  And just like that, we’ve defined three unique implementations for a form validation interface, without having to define three implementing classes.  Each method implementation contributes to the application in the same way, but operates differently based on the specific implementation.  Lambda expressions do not have to be so succinct as they are in the above snippet.  They can be multiline method bodies that return values.  You can even pass a functional interface as a parameter to another method, each time defining a new lambda expression to modify the behavior of that functional interface as a parameter.  For more information on this, check out this well-written post on Medium.   To wrap up Java 8, it is worth mentioning default interfaces, type-based annotations, Optional objects, and Stream collections.   Default interfaces are nothing more than interfaces whose method bodies have a default implementation.  This eliminates the need for abstract parent classes which define default behaviour for the interface, allowing all child classes to defer to super.  Now abstract parent classes will exist if and only if there is a good reason for them, keeping code neat, organized and easy to read.   Type annotations are annotations that are specified at the field level.  Type annotations are very popular in the Hibernate and JPA libraries, where you can define database relationships between two records on a foreign key.   Optional objects can be assigned null values without throwing NullPointerExceptions.  These objects do much in the way of cleaning boiler plate code.  Instead of checking if an object is equal to null, you can use an Optional type which contain constructs like Optional.empty or Optional.isPresent to replace your constant null checking.   Streams are Collections that have been modified for functional use.  Consider the List collection, typically a LinkedList or an ArrayList data structure.  These data structures were designed for traditional CRUD operations in a very object-oriented way.  But what happens when you need to iterate over all of elements in a list that meet a criteria, perform some chain of operations against those elements, and forward those elements on to a new list object?  You could spend a lot of time working on a “helper object” that is designed specifically to modify all of your list objects in your application to accomplish this, or you could use stream operations.  Streams have been integrated into many of the native Java data structures (just like many lambda functions) allowing them to be easily pulled from pre-Java 8 code.  Consider the following example pulled from this excellent resource on the Streams API  List&lt;String&gt; myList = Arrays.asList(\"a1\", \"a2\", \"b1\", \"c2\", \"c1\"); Stream&lt;String&gt; myStream = myList.stream();  myStream()     .filter(s -&gt; s.startsWith(\"c\"))     .map(String::toUpperCase)     .sorted()     .forEach(System.out::println);  // C1 // C2  We create a List object and from that List we create a Stream.  The filter method takes a lambda that pulls all elements out of the stream which start with a “c”.  The map method passes the method reference for the toUpposerCase method, which is part of the String object, and applies it to each element in the Stream at this time (which is just c2 and c1).  The sorted method sorts the elements in the stream in alpha-numeric order (resulting in C1 and C2).  Finally the forEach method takes another method references which returns the type void and applies it to the remaining members of the stream, thus printing the strings “C1” and “C2”.  Streams are another departure from traditional Java, much like lambda expressions.  They are very functional tools that cannot be re-used after a terminating operation has been called on it (i.e. any method which returns void, in the above case the forEach method).  However, they are invaluable tools for parsing large data sets.  Whole classes designed to process large data structures can be reduced to a few lines of code using streams.   Java 9 Non-LTS  Java 9 improved on some of the features introduced in Java 8, but not to such a degree that required it be an LTS release.  Some of the features like private interface methods, immutable sets defined in one-line, a new Garbage Collector, a new HTTP Client, and enhancements made to the @Deprecated annotation did not really do much to move the language forward.  These features deserve to be mentioned, but they did not shake up the Java development world the way functional interfaces and lambda expressions did for Java 8.  That being said, Java 9 is not without its bells and whistles.   The jshell command line tool was introduced in Java 9.  This tool, allowed users to flesh out Java code in the command line without having to set up a brand-new Java project with the inescapabale public static void main(String[] args) method.  Developers could now write Java code in the command line at will.  This brought Java one step closer to Python; Python always had a CLI component and now so too did Java.  The jshell is a whole topic on its own, and it exceeds the scope of this blog post.  For more information on this tool though, check out the Oracle documentation on it.   Another component to Java 9 that is worth mentioning is the introduction of modules.  Modules introduce a re-organization of Java APIs, by grouping common libraries and specifying groups of dependent libraries.  These groups create modules, and they define an entirely new way to build applications.  Using the javac compiler, you can specify a module-info.java along with any relevant class files.  What results is a module that you can run in the command line via the java command.  You no longer need to compile JAR files or memorize the different program entry points you have in your JAR or your manifest file.  For example, given the class HelloWorld belonging to package com.mydomain, you can write a module-info.java file containing the line module com.mydomain {}.  Then with the following command, you can compile a java module  javac -d mymodule/myproject src/myproject/module-info.java src/myproject/com/mydomain/HelloWorld.java java --module-path mymodule -m myproject/com.mydomain.HelloWorld  Modules belong to a wider initiative known as Project Jigsaw, which again extends beyond the scope of this blog post.  For information on modules, how they work, and Project Jigsaw, check out this article   Java 10 Non-LTS  Java 9 was the release that re-defined how Java programs could be organized and developed.  Java 10 was the release that built upon the foundation laid in Java 9.  The most subtle, but most important feature of this release were the performance enhancements introduced to the jshell tool.  The jshell tool was a powerful addition in Java 9, but it took awhile for it to get going.  Java 10 kicked it into gear a lot faster than Java 9 did, introducing performance enhancements that made the tool more desirable to use for developers.   Additionally, Java 10 improved upon Java 9’s new Garbage Collector, the Garbage-First Garbage Collector.  Java 9’s G1GC changed the way Java performed garbage collecting by partitioning its heap into multiple small heaps, whereas traditionally the heap would be split into three chunks.  G1GC also introduced multi-threaded processing on the garbage collector for all events save a “stop-the world,” a point in time during garbage collection where program execution halts and gives the garbage collector time to assess the state of the application.  “Stop-the-world” events are unavoidable in garbage collection.  Java 10 took the multi-threaded approach perfected by Java 9 and applied it to the “stop-the-world” event, thus making the G1GC fully concurrent.  Even though the G1GC is fully concurrent and therefore faster than prior garbage collectors on average, it is worth noting it takes a larger memory footprint because of the way it chops the heap into more chunks.   The big language feature introduced in Java 10 is the var keyword.  The var keyword intoduces implicitly typed variables to Java.  The concept of the implicit type has been around forever; Python and Perl use implicit typing out of the box, as does C# (with reflection or the var keyword).  Java has always traditionally been a statically typed language; the closest to inferred types you could get before version 10 was with Generic classes and runtime casting.  With the introduction of the var keyword though, we can infer an object’s type at compile time, making code easier to read.  Java is still a statically typed language, nothing will change that.  But the introduciton of the var keyword extends the usability of Java to use cases where developers may choose a dynamically typed language by default.  By simply introducing the concept of an inferred type, Java, a statically typed language, can have some of the use cases that make dynamically typed languages so valuable.  This is an exciting potential use-case for Java.  The use of the var keyword, combined with tools like the jshell, propel Java into the functional programming space.  That being said, I would caution excited fans of inferred types to use var only when necessary.  The following is a perfect example:  MyFakeMongoDBUtilityClass.StaticInnerClassOfMyFakeClass&lt;FakeKey, FakeValue&gt; myStaticallyTypedObject = new MyFakeMongoDBUtilityClass.StaticInnerClassOfMyFakeClass&lt;FakeKey, FakeValue&gt;(); var myInferredTypedObject = new MyFakeMongoDBUtilityClass.StaticInnerClassOfMyFakeClass&lt;FakeKey, FakeValue&gt;();  As you can see, there are obvious readibility use cases for the var keyword.  However, this keyword comes with a few caveats.  For example, var cannot be used to assign a type to class-level variables, method parameters, or method return types.  These values need to be known at all times, otherwise your application will not compile.  Furthermore, you should not use var when instantiating a variable that holds a value returned by a method.  Doing so prevents your code from being readable.  For example:  // Do this MyFakeMongoDBUtilityClass.StaticInnerClassOfMyFakeClass&lt;FakeKey, FakeValue&gt; myMethodReturnObject = someMethod(); // Do NOT do this var myMethodReturnObjectOfUnknownType = someMethod();  One thing to note about var is it is not reserved.  This means you could feasibly have a situation like var var.  This feature is designed to protect the lazy programmers out there who have created variables named “var.”   Java 11 LTS  Java 11, the most recently released Java version, is the second LTS Java version released by Oracle.  This version continued where Java 10 and 9 left off.  Modifications were made to the ever popular var keyword, allowing them to be used within lambda expressions.  This means you can now have variables within your lambda expression that are locally-scoped to that lambda expression only.  This is just another feather-in-the-cap for lambdas; the integration of functional paradigms and inferred types shows a real embrace of modern programming styles.  Another great feature of Java 11 is the single line, single-file build.  You can now run the following without having to compile an entire application.  java HelloWorld.java  If you want to test something out quickly, you could mock up your application using jshell.  Then you can take the commands you painstakingly tested and copy/paste them into a poc.java file.  This can be attached into an email for your co-worker and forwarded to them.  They can then run the application in one line and, assuming you are using native libraries only, the application will run perfectly.  This is truly a big step for Java; the developers working on the JDK have worked tirelessly to make Java a language that can stand the test of the modern programming paradigm.   Languages like Python have always been used to write enterprise level applications, just like Java and C++.  However, Python has always had an edge in that it can be used in a highly functional, almost ad-hoc way.  Until Java 9, this was impossible.  Now with Java 11 introducing single-file compilation under an LTS release with lambda functions that have locally scoped, inferred type variables, companies can almost replace their Python scripts with Java scripts.  This is a huge win for Java.  Take for example the machine learning world.   Every tool used to automate model training and generation will generate a reference to the generated model as either a python script or a POJO (plain-old Java object).  The Python machine learning shops out there had a realy easy time testing deploying their model, they just drag and drop the file to the location where there application stack calls the model and it was deployed.  The Java machine learning shops would have to take the POJO and integrate it into their application, re-compile everything, and re-start the Java process running on their ML machine.  For some production environments, this may only be allowed on a weekend with the coordination of multiple teams.  Now, with Java 11 and single-file compile, deploying a new model in Java is as easy as deploying a new model in Python has always been.   Future Directions and World Domination  Java has had quite the glow-up in the last 5 years.  Java has gone from a language on the cusp of becoming one of the “old-school” languages like C or Fortran, and has been re-made in the image of languages that are succeeding today for modern use-cases.  The great thing about Java used to be it was easy to learn.  There are no function pointers, there is only one type of casting, no multiple inheritance, you could run Java on any platform, and you do not need to know how to use any command line tools because your IDE takes care of most of it.  If you are a novice, the command line in your Eclipse IDE is how you interact with the application.  If you were curious, you built a JApplet.  If you were an enterprise developer, you built a REST interface underneath some HTML you copy/pasted off the web.  This worked for awhile, until our data sets got bigger, our databases all became columnar, our front-ends were stand-alone products like Grafana, and we became more interested in our future results over our current results.  How does a language like Java stand a chance in a world where the function of the object outweighs the object itself?  The short answer is, Java needed to be reinvented.  Now we have a robust object-oriented language that can easily fill the role of a functional scripting language.  By trimming the fat inherent to every Java application, Java once again becomes an accessible language.  We no longer need 20 lines of import statements, we can just make a module.  We no longer need to compile thousands of lines of code to see if one crucial piece works as expected, we can just use jshell.  We no longer have long-winded anonymous classes that define highly specific types (yes I am talking about you Apache HTTP Client), we have lambda expressions and inferred types.  We no longer have helper classes designed to troll through your data structures for entries that meet specific criteria, we have the Streams API.  Pretty soon, the iconic POJO may be replaced with a concept called a record in Java 12.  This initiative to keep Java modern ensures it will be used for many more years to come.   I only hope the powers that be start to realize C++ is the next candidate for the same treatment.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/comparing-java-lts-releases",
        "teaser": null
      },{
        "title": "Using Swagger to Build Effective APIs Part 1",
        "excerpt":"The rise of serverless computing and platform agnostic, app-based services use has forced the casual developer to learn proper API development, a task usually reserved for platform architects.  Unfortunately, there are many programmers, start-ups, and full-fledged companies that do not adhere to proper API development and maintenance. The skillsets and mindsets involved in creating APIs are different from those involved in developing the business logic which uses an API.   This schism does not affect developers or architects; it only affects the users of the API.  Poorly developed or poorly documented APIs which are susceptible to frequent changes or misleading design decisions are difficult to use.  If a product is difficult to use it likely will not be used at all, especially if that product is an API.  The good news is there are specifications and tools which aid in the development of easy to use APIs that change as fast as your business.  I am speaking specifically about the Open API Specification and Swagger.   OpenAPI Initiative and Swagger  As REST APIs became more prevalent across the Internet, the standards used to develop them became more varied.  To fix this issue, a bunch of companies got together and created the OpenAPI Initiative.  This initiative sought to codify and finalize a standard ruleset for REST API development.  Since the inception of this open source group, the OpenAPI Specification (OAS) has gone through 3 versions, the last two of which are fully supported by the Swagger API Editor.   Swagger is a company that creates and supports open source API development tools.  Many of these tools are available on their website, or as we will shortly see as Docker containers.  These tools like Swagger Hub, Swagger Editor, Swagger UI, and Swagger Validator all work together to aid in developing proper APIs.  Furthermore, all Swagger tools support the OAS 3.0.n specification.  This fact makes adhering to OAS standards very easy when using Swagger tools.   The exact rules defining OAS are on Github for any user to peruse, but tools like Swagger make conforming to these rules easy for anyone.  You do not have to be familiar with every detail of the OpenAPI specification to develop an API which conforms to said specification.  The rules themselves are very detailed, and describing each one in this blog post would be redundant.  Instead, we are going to use Swagger to build a demo REST API which conforms to OpenAPI standards and syntax.  After walking through this tutorial, I encourage the reader to browse the specification in more detail.  I will cover as many features of the OpenAPI version 3.0.0 specification as possible in this article, but the most detail can be found in the specification itself.   What is this API going to do?  Every API needs to have a purpose which defines how it will be used.  Personally, I love to exercise.  Going to the gym, taking a fitness class, and running are all ways I like to workout.  But as my workouts become more varied, it is difficult to track my progress from workout to workout.  We are going to develop a sample API which will help me track my exercises, no matter how varied they are.   Take a look at this graphic:      Here we define a workout as an array of exercises.  Each exercise is a combination of sets with a rest time.  Each set is an array of repetitions defined primarily by a weight, as well as a lift, which is defined primarily by an affected muscle group.  Now we have the framework to easily define a weight lifting workout.  How do we translate this diagram of loose fitting parts into an API?   There are three steps we will need to take to turn these UML diagrams into tangible JSON packets.  The first step is to set up our local Swagger development environment.  The second is to populate our API definitions with some sample paths.  The last step is to test these sample paths using Swagger’s testing tools on Swagger Hub.  It is important to note that because we are using Swagger tools to develop our API, we will automatically be prescribing to OAS 3.0 standards, ensuring our API will be easy to use for developers.   Setting up Swagger-Editor  If you prefer to work entirely online, feel free to check out Swagger.io.  Their service, Swagger Hub, functions exactly like Swagger Editor, and it even includes testing features which we will explore later.  Personally, I prefer to use Docker containers so I do not always have to be connected to the Internet.  If you do decide to use Swagger Hub, you can skip over this portion of the post.  MacOS  For MacOS, install Docker on your laptop by running the following in a terminal:  brew install docker  Windows  For Windows, navigate to the Docker download link and follow the prompts until Docker is installed.  Next Steps  Once Docker is installed, run the docker search swagger command using the docker CLI to search for the swagger editor container.  It is important to note here, we want the swaggerapi/swagger-editor image.  The swagger-ui image is better for presenting API documentation in a web browser.  The swagger-generator image starts a web server that will generate clients or servers in a RESTful context when given an API as input.  The swagger-validator image is used for assigning Swagger badges to Github repositories.  In this instance, we only care about the swagger-editor image, as it will allow us to deploy a local web service that we can use to edit our API without logging into Swagger’s online services.  $ docker search swagger NAME                                     DESCRIPTION                                     STARS               OFFICIAL swaggerapi/swagger-editor                The swagger-editor web service                  240 swaggerapi/swagger-ui                    A simple docker container for hosting swagge…   145 swaggerapi/swagger-generator             The swagger codegen web service                 66 ...... $ $ $ docker pull swaggerapi/swagger-editor  Once you have your container downloaded, run the following command to start the container and map the container port 8080 to your computer’s port 8080  $ docker run -p 127.0.0.1:8080:8080 swaggerapi/swagger-editor &amp;  This should start the container, but we can confirm by running docker ps which should give output similar to the following:  $ docker ps CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS                  PORTS                      NAMES 258a99d971e9        swaggerapi/swagger-editor   \"sh /usr/share/nginx…\"   1 second ago        Up Less than a second   127.0.0.1:8080-&gt;8080/tcp   elated_margulis  By navigating to port 8080 in our browser, we can see the default Swagger Editor UI.  You should see the standard Pet Store sample API definition in the Swagger UI:      The Pet Store API is the sample API for the Swagger 2.0 specification.  In the next section, we’ll explore how we can convert this API into an OpenAPI 3.0 Specification API, in accordance with the UML diagrams above.   Swagger-Editor vs. Swagger Hub  If the above steps were at all difficult or prohibitive for you, check out Swagger.io.  This website hosts all of the web services discussed here in their newest versions.  The only drawback is you will have to login to the Swagger website and host your API there.  If this is not a problem for you, feel free to use Swagger Hub for the remainder of this tutorial.  Both platforms feature Ctrl+Space code completion tips that are essential if you find yourself struggling to format your API properly.  We will be using Swagger Hub later in this post to test our API.   Configuring Swagger for OpenAPI 3.0 Specification  The default Swagger file in the Swagger Editor conforms to Swagger 2.0 (also known as Open API Specification 2.0).  The Swagger Editor supports the generation of clients and servers in nearly any language for the given API so long as that API conforms to the 2.0 specification.  This alone is a strong impetus to retain the 2.0 API specification; but since the goal of this project is to eventually build an API and program its functions using serverless technology, we do not need to generate client and server code to handle API calls.  Even so, it should be noted for those of you using Swagger Hub, the online tool, you can still generate clients and servers against OAS 3.0, you just cannot generate clients and servers against OAS 3.0 using the Swagger Editor container.   API Metadata &amp; Additional Docs Section  The first component of an OAS 3.0 API is not relevant to the function of the API.  This section of an API specification retains information about the API like which version it is, who wrote the API, servers where you can contact the API, additional documentation, licensing information, and the APIs schema.  In our case, this block of YAML should look similar to the following:  openapi: 3.0.0 info:   title: ASpotr   version: 1.0.0   description: This is the ASpotr API.   contact:     email: dferguson@ippon.fr     name: Daniel Ferguson     url: 'http://dferguson992.github.io'   license:     name: Apache 2.0     url: 'http://www.apache.org/licenses/LICENSE-2.0.html' servers:   - url: 'http://www.aspotr.com'     variables: {}     description: ASpotr API Gateway, ASpotr is serverless externalDocs:   description: Find out more about Swagger   url: 'http://swagger.io'  The first line defines the specification this API adheres too.  This will usually be swagger: 1.0, swagger: 2.0, or openapi: 3.0.n where n is some minor version number.  Here I use 3.0.0 for simplicity’s sake.  This first line tells the swagger validator what to expect in the rest of the document, much like defining XSD files in an XML file.  The next block, info, is for information about the API.  This is an optional component, but it is good practice to specify this block, as well as servers and externalDocs anyway.  This entire section is completely optional, but a good API, especially an open source one, will maintain license and contact information for use by contributors.   Components Section  The second major component to an OAS 3.0 API are the components.  For those of you with an object-oriented programming background, it is easy to think of components as plain objects without any business logic.  In order to build strong path definitions in your API, it is imperative we build strong, well-defined models that can be referenced from your paths.  This is where the UML diagram from above comes in handy.  Considering the UML diagram, we can create API models defined below.  As you read this section, pay close attention to the use of the $ref tag:  components:   schemas:     MuscleGroup:       type: object       properties:         id:           type: integer           format: int64         name:           type: string           description: The name of the Muscle Group     Lift:       type: object       properties:         id:           type: integer           format: int64         name:           type: string           description: The name of the Exercise         description:           type: string         primaryMuscleGroupId:           $ref: '#/components/schemas/MuscleGroup'         movementModifier:           type: string           description: &gt;-             Select these modifiers to further describe the exercise you're             doing.           enum:             - isolation             - supination             - wide             - narrow       required:         - id         - name         - primaryMuscleId     Repetition:       type: object       properties:         id:           type: integer           format: int64         weight:           type: integer           format: int64         system:           type: boolean           description: English(T) or Metric(F) units         repetitionModified:           type: string           description: &gt;-             Select these modifiers to further describe the repetition you're doing.           enum:             - tempo             - static             - plyometric             - isometric       required:         - id     Set:       type: object       properties:         id:           type: integer           format: int64         exercise:           $ref: '#/components/schemas/Lift'         repetitions:           type: array           items:             $ref: '#/components/schemas/Repetition'         name:           type: string           description: The name of the set.  Should be autogenerated by the API based on the information within the set.       required:         - id         - exercise         - repetitions     Exercise:       type: object       properties:         id:           type: integer           format: int64         sets:           type: array           items:             $ref: '#/components/schemas/Set'         avgRestTime:           type: integer           format: int64       required:         - id         - sets     Workout:       type: object       properties:         id:           type: integer           format: int64         exercises:           type: array           items:             $ref: '#/components/schemas/Exercise'       required:         - id         - exercises  Starting with our most primitive objects built on basic data types like MuscleGroup, we can define complex objects like Workout.  The glue that binds these objects together is the $ref tag.  This tag references another location in your API specification.  When defining models, we use it heavily to build our complex objects based on definitions of simpler objects.  This tag works by defining a structure similar to directories, where the # character is the root tag, and the / character separates each level of the YAML document.   This structure helps to define components in your API using other, previously defined components.  This tag is incredibly powerful, as it allows you to not only reference components in this document and also reference components in other API specification documents as well.  This is a great strategy if you are linking two or more extraneous APIs, or if you want to keep your component descriptions separated from the rest of your API document.   It is important to note that the demo API in this post may not be a scalable API design.  For our purposes, this design may work just fine; it is glaringly obvious this design will create large, nested JSON payloads for even the shortest workouts.  The example API in this post implies each transaction is stateful, and payload components are dependent upon other potentially unrelated components.   For example, say you want to track the number of push-ups you’ve done in a day.  With this API design, you have two choices.  The first is to slowly build your JSON workout object by periodically adding set objects to it throughout the day.  The second option is to send small workout requests objects throughout the day, each containing one exercise of about 20 or so push-ups.   The first option is not convenient, but it lends itself well to exercise tracking, mainly if you wanted to model the data you’ve collected and build progress projections over time.  This first approach is how one would assume any workout tracking is to be done.  The second approach is much more convenient for the user, even if the machine learning conducted on your data set is blown out of the water if multiple workouts of the same exercise are logged per day.   For our purposes now, this API will work just fine; but as you continue to read, consider optimizations to the API that would make it scalable and easy to use.  It is essential to think about the API from all perspectives, those modeling the business logic on the back-end, and those submitted REST requests on the front-end.  If you keep these tenants in mind, you can build a much more atomic API that maintains its usability by both developers and users.   Summary  This post is the first in a series discussing API development for serverless applications.  In the next post, we will discuss developing API paths in your Swagger specification, and how to test that API specification using Swagger tools.   If something in this article piqued your interest and you would like more information on JHipster or the services we offer at Ippon USA we’d love to hear from you! Please contact us at contact@ippon.tech with your questions and inquiries.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/using-swagger-to-build-effective-apis-part-1",
        "teaser": null
      },{
        "title": "Using Swagger to Build Effective APIs Part 2",
        "excerpt":"This post is a continuation in a discussion on developing APIs which prescribe to the Open API 3.0 specification using Swagger tools.  In this post, we will continue our dicussion with writing paths for your API and testing those paths using Swagger Hub.   Paths Sections  The third and most important component to an API specification is the paths section.  Here is the meat and potatoes of the API, the definitions behind how it will be used by developers in the future.  Let’s look at a simple example of two paths this API could take for the exercise resource:  paths:   /exercises:     summary: Returns a paginated list of exercises     description: Returns a paginated list of exercises     get:       tags:         - Exercise       summary: 'Sample GET operation on the /exercises endpoint.'       description: 'This operation, defined by the noun \"exercises\" and the verb \"GET\" implies we will be making an HTTP GET request against the API, will receive multiple Exercise objects in the response.'       operationId: 'getAllExercises'       responses:         '200':           description: Successfully retrieved paginated list of all Exercise objects           content:             application/json:               schema:                 type: array                 items:                   $ref: '#/components/schemas/Exercise'         '500':           description: Server error prevented successful retrieval   /exercises/{id}:     summary: Returns an exercise based on the ID passed     description: Returns an exercise based on the ID passed     get:       tags:         - Exercise ID       summary: \"Returns an exercise based on the ID passed\"       description: \"Returns an exercise based on the ID passed\"       parameters:         - name: id           in: path           description: Exercise ID           required: true           schema:             type: integer             format: int64       operationId: 'getExerciseByID'       responses:         '200':           description: Successfully retrieved a single Exercise object           content:             application/json:               schema:                 $ref: '#/components/schemas/Exercise'   /exercises/exercise:     summary: POST a new exercise     description: POST a new exercise     post:       tags:       - Exercise       operationId: 'postExercise'       summary: \"POST a new exercise\"       description: \"POST a new exercise\"       requestBody:         required: true         content:           application/json:             schema:               $ref: '#/components/schemas/Exercise'       responses:         '202':           description: Payload was accepted           content:             application/json:               schema:                 $ref: '#/components/schemas/Exercise'         '400':           description: Back-end was unavailable  The example above shows two GET endpoints that define operations on the exercise resource defined in the components section of the specification.  You’ll notice for each endpoint we define an HTTP Method (at least one), any relevant parameters associated with that method request, and the expected responses that come from that request path are mapped to components with the $ref tag.   Pathing is a complex topic in API development.  There are many things to consider when defining API paths, like HTTP Headers, HTTP Methods, endpoint names, response types, versions, etc.  For example, you will notice each API path I have specified is a noun.  This noun is either singular or plural, depending on the number of objects returned by the response definition.   The only verbs in this API design are the HTTP verbs, which is as it should be.  Some developers may be tempted to say http://www.myapi.com/getAllExercises.  This is not good practice; behind the scenes, this operation is a GET getAllExercises, which is redundant.  Instead, define your API paths as nouns and use the HTTP operations as verbs.   If you can concatenate the two and create a sensible phrase, you have successfully defined a logical endpoint like POST exercise or GET exercises.  The use of verbs to define REST API operations is a tricky business and is generally considered bad practice.  However, if you are defining a different kind of API, like SOAP or more generally RPC, this is good practice.  RPC APIs are beyond the scope of this post, but if you would like to do more reading, I would suggest this article, which neatly defines the differences between REST APIs and RPC type APIs.   Usually, APIs are written by architects, for developers.  This immediately implies a paradigm shift will be required in order to keep your API future proof from developer requests.  If you want to design your API so it is easily upgradeable, easily maintainable, and easily understood by both users and developers, check out these articles: API Design and API Best Practices.   Test your Swagger using Swagger Hub tools  Once you’ve built your API Specification using the Swagger Editor, you have to test it.  By far, the easiest way you can test your API specification is to copy it from the Swagger Editor and paste it into Swagger Hub.  This will, of course, require you to sign-in to Swagger Hub and create a new API; but the ease of testing involved make the process well worth it.  Additionally, you can make any new API private on Swagger Hub, if you are concerned others may poach your API.   Once you’ve pasted your API into the Swagger Hub, you’ll notice the UI will take on a very similar look and feel as the Swagger Editor.  They are not identical displays however; in fact, you may notice that Swagger Hub adds a server line for you.  This is what Swagger Hub added to the server section of my API  # Added by API Auto Mocking Plugin   - description: SwaggerHub API Auto Mocking     url: https://virtserver.swaggerhub.com/dferguson992/aspotr/1.0.0  This is a virtual server used to mock requests to your API.  Your username, API name, and API Version number all define the URL of the mock server.  You cannot ping the server, but you can run curl commands against API endpoints hosted on the server.  These API endpoints are pulled straight from your specification and will send mock data to your endpoints so you can physically see the responses.  Furthermore, you are free to modify the mock data to fit any edge cases you may want to program into your application in the future.  It is important to note, this is just a sanity test designed to allow you to see the specified output of your API.  You should not use the mock endpoints to actually test your business logic.  The mock endpoint is created solely for the purpose of viewing and verifying the expected output.   Let’s look at a few of the examples from the demo API:  $ curl -X GET \"https://virtserver.swaggerhub.com/dferguson992/aspotr/1.0.0/exercises\" -H \"accept: application/json\" [ {   \"id\" : 0,   \"sets\" : [ {     \"id\" : 0,     \"exercise\" : {       \"id\" : 0,       \"name\" : \"string\",       \"description\" : \"string\",       \"primaryMuscleGroupId\" : {         \"id\" : 0,         \"name\" : \"string\"       },       \"movementModifier\" : \"isolation\"     },     \"repetitions\" : [ {       \"id\" : 0,       \"weight\" : 0,       \"system\" : true,       \"repetitionModified\" : \"tempo\"     } ],     \"name\" : \"string\"   } ],   \"avgRestTime\" : 0 } $ curl -X GET \"https://virtserver.swaggerhub.com/dferguson992/aspotr/1.0.0/exercises/1\" -H \"accept: application/json\" {   \"id\" : 0,   \"sets\" : [ {     \"id\" : 0,     \"exercise\" : {       \"id\" : 0,       \"name\" : \"string\",       \"description\" : \"string\",       \"primaryMuscleGroupId\" : {         \"id\" : 0,         \"name\" : \"string\"       },       \"movementModifier\" : \"isolation\"     },     \"repetitions\" : [ {       \"id\" : 0,       \"weight\" : 0,       \"system\" : true,       \"repetitionModified\" : \"tempo\"     } ],     \"name\" : \"string\"   } ],   \"avgRestTime\" : 0 }  As you can see, these two GET requests return basic Exercise objects.  This mocking allows us to view the exercises as they would be returned from the API.  This is important, as it allows us to easily define improved endpoints for future releases of the API.  Seeing the results of the endpoint requests, even if they are mocked, will always be more valuable than writing the specification and never seeing it in action until business logic is written.   We can even modify the request body of the POST endpoint we defined earlier:  $ curl -X POST \"https://virtserver.swaggerhub.com/dferguson992/aspotr/1.0.0/exercises/exercise\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d {\"id\":1,\"sets\":[{\"id\":0,\"exercise\":{\"id\":0,\"name\":\"stricription\":\"string\",\"primaryMuscleGroupId\":{\"id\":0,\"name\":\"string\"},\"movementModifier\":\"isolation\"},\"repetitions\":[{\"id\":0,\"weight\":0,\"system\":true,\"repetitionModified\":\"tempo\"}],\"name\":\"string\"}],\"avgRestTime\":0} {   \"id\" : 0,   \"sets\" : [ {     \"id\" : 0,     \"exercise\" : {       \"id\" : 0,       \"name\" : \"string\",       \"description\" : \"string\",       \"primaryMuscleGroupId\" : {         \"id\" : 0,         \"name\" : \"string\"       },       \"movementModifier\" : \"isolation\"     },     \"repetitions\" : [ {       \"id\" : 0,       \"weight\" : 0,       \"system\" : true,       \"repetitionModified\" : \"tempo\"     } ],     \"name\" : \"string\"   } ],   \"avgRestTime\" : 0 }  We defined the payload as an exercise containing just default values for each field, and passed it in JSON to the curl statement under the -d flag.  As you can see, our API returned a sample exercise object that deviated from our payload.  It is difficult to see at first, but if you look closely you will see we passed an exercise with an id of “1” to the curl statment, and received an exercise with an id of “0” in the response body.  This is a short-coming inherent to mocking.  In our API specification, we may have intended our API to return the exact response we sent, but there is no way to emulate this behavior in a mock.  This is because REST APIs are stateful.  There is no way for us to emulate this intended behavior without designing business logic.  Situations like these are important to pay close attention to when mocking an API, as they may reveal flaws in your assumption about how your API will work.   Summary  Swagger is a great toolset for anyone looking to develop an API.  For those of us looking to write APIs, you should always adhere to a common standard which will make your API usable.  That is why you should use the OAS 3.0.n specification for your API.  APIs that do not adhere to this design have the potential to be difficult to use and maintain.  Any tool that is difficult to maintain will not be used.   Inversely, Swagger is a tool that is very easy to use, which is why I have developed this demo API using Swagger tools in the first place.  Some of the additional benefits of Swagger tools include their portability.  Swagger Hub will let you generate templated client and server code in nearly any language you want.  This allows you to rapidly prototype APIs for the future.   For example, AWS API Gateway will generate all of the features of your OAS compliant API like models and routes, will help you test your API, and will even deploy your API in a serverless context using AWS Lambda to handle endpoint calls.  The only thing API Gateway needs to get started is a Swagger document.  Maven has a swagger plug-in which takes a definition YAML file and generates client models on the fly so you can rapidly prototype an application for your API.  If the API changes, you just update the definition file and your API code will change with it.  Swagger is the toolset for helping architects write APIs quickly, and OAS 3.0.n allows these APIs to be used by everyone.   If something in this article piqued your interest and you would like more information on JHipster or the services we offer at Ippon USA we’d love to hear from you! Please contact us at contact@ippon.tech with your questions and inquiries.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/using-swagger-to-build-effective-apis-part-2",
        "teaser": null
      },{
        "title": "Using Swagger to Build Effective APIs Part 3",
        "excerpt":"This post is a continuation in a discussion on developing APIs which prescribe to the Open API 3.0 specification using Swagger tools.  In this post, we will continue our dicussion with adding security to your API and testing your security configurations using Swagger Hub.   Security Schemes  Recall from the first post in this discussion the section on schemas to describe our API models.  These schemas lived under the components section of an OAS 3.0 compliant API.  Security schemes are deployed in the same way.  Using OAS 3.0 notation, we can define multiple security schemes underneath the component section of your API.  Once you’ve defined your security schemes, you apply these schemes using the security tag, either at the root of your API, or against individual paths.  The nesting quality of the security tag allows you to customize access to your API in a very deterministic and detailed way.      Additionally, some security schemes come with customizable “scopes” (think read or write, admin or guest) which we can apply to our API and endpoints in the same way.  This provides for more granular security across our API.  We’ll explore exactly what I mean using the workout API from the previous posts; but first, let’s discuss the different OAS 3.0 supported security paradigms and how we can “scope” them to API paths and apply roles to our security.   Basic Authentication  Basic Authentication is the most primitive authentication scheme available to you.  If you’re familiar with the HTTP protocol, you’ll know this scheme is built straight into HTTP using the “Authorization” header.  The username and password are concatenated with a “:” and are converted into their base-64 encoding.  The resulting mish-mosh of characters, along with the “Basic” specifier, is your authorization token.  There’s not much else to discuss here, especially as it applies to OAS 3.0.      Defining Basic Authentication in your security schemes takes the form  components:     securitySchemes:         basicAuth:     # &lt;-- arbitrary name for the security scheme           type: http           scheme: basic  If you were to employ Basic Authentication against an endpoint, it would take the form  paths:   /something:     get:       security:         - basicAuth:[]  Please notice two important things here.  First, the security tag mentions basicAuth as a scheme.  This is not a keyword, nor does OAS 3.0 recognize this string as being Basic Authentication.  The only reason the string basicAuth is associated with Basic Authentication is because of how it is defined above in the security schemes section.  Within the basicAuth tag we define scheme: basic as the scheme.  The scheme key is how you truly specify which authentication scheme you are employing; the name of the scheme is irrelevant, and it serves only to identify that scheme for future use in your API.     The second important detail to notice in this example is the square brackets after the basicAuth under the security tag.  This is semantically required by OAS 3.0 to support the “scoping” of authentication schemes.  Because Basic Authentication is so simple, we do not have to worry about scopes when using it.  That is why the square brackets are empty.  We will see scopes being used in more detail later in this post, but for now it is important to recognize where they are used and why this notation exist.   Bearer Authentication  Bearer Authentication is a token-based HTTP scheme which similarly employs the Authorization HTTP header.  This scheme works similarly to the Basic Authentication scheme, except instead of “Basic base64 encoded username:password” being passed to the Authorization header, the string “Bearer token” is passed to the Authorization header.  This token is usually disseminated to users in an e-mail after they have confirmed their intent to use your API, or after they have paid for a service plan.  You can specify the use of Bearer Authentication in your API using the OAS 3.0 notation below.  components:   securitySchemes:     bearerAuth:            # arbitrary name for the security scheme       type: http       scheme: bearer       bearerFormat: JWT    # optional, arbitrary value for documentation purposes paths:   /something:     get:       security:         - bearerAuth: []  Notice again the scheme name serves only as an identifier for the scheme later on when applied to an endpoint.  Additionally, this scheme has no scope, and so therefore has empty square brackets when it is employed as a security scheme against an endpoint.   API Keys  Think of API Key authentication schemes as a more extensible Bearer Authentication scheme.  Where Bearer Authentication is passed via the request header only, API Keys can be passed in the request body, the request header, or as a cookie.  # 1) Define the key name and location components:   securitySchemes:     ApiKeyAuth:        # arbitrary name for the security scheme       type: apiKey       in: header       # can be \"header\", \"query\" or \"cookie\"       name: X-API-KEY  # name of the header, query parameter or cookie # 2) Apply the API key globally to all operations security:   - ApiKeyAuth: []     # use the same name as under securitySchemes  As was the case with Basic and Bearer Authentication, API Keys do not have API based scopes, hence the square brackets.  This makes sense because API Keys are traditionally provisioned from the APIs backend.  The backend logic which provisions these API keys will keep track of permissions associated with the keys.  Therefore, keeping track of key permissions via scopes at the API layer is not only redundant, but could be completely wrong.  There’s no way your API can reflect in its specification the permissions associated with a backend generated key.  If we consider an API Key a glorified username and password or a bearer token, this logic applies transitively to Basic and Bearer Authentication schemes.  In short, the empty brackets defining scope are reserved for our last scheme, OAuth2.   OAuth2 Connect  The below ASCII art taken from the OAuth 2.0 RFC succinctly summarizes the basic idea behind how OAuth 2.0 will authorize access to a resource.  +--------+                               +---------------+ |        |--(A)- Authorization Request -&gt;|   Resource    | |        |                               |     Owner     | |        |&lt;-(B)-- Authorization Grant ---|               | |        |                               +---------------+ |        | |        |                               +---------------+ |        |--(C)-- Authorization Grant --&gt;| Authorization | | Client |                               |     Server    | |        |&lt;-(D)----- Access Token -------|               | |        |                               +---------------+ |        | |        |                               +---------------+ |        |--(E)----- Access Token ------&gt;|    Resource   | |        |                               |     Server    | |        |&lt;-(F)--- Protected Resource ---|               | +--------+                               +---------------+  OAuth 2.0 is widely regarded as the standard for authorization.  It is a complex paradigm whose details are beyond the scope of this article.  For more information, check out the OAuth 2.0 information page.      A quick summary of OAuth2 as it applies to OAS 3.0 requires the understanding of grant types, referred to as flows in Swagger notation.  These flows define actions which a seperate authentication server will authorize against your API.  This decouples authorization from your API.  OAS 3.0 has supports several OAuth2.0 flows, each flow having its own purpose and sub-key API specifications.  For a detailed look at how OAuth2.0 is specified using OAS 3.0, I strongly recommend checking the documentation.  This article is about using Swagger tools to secure your API, not about the finer points of OAuth2.0.     It is worth mentioning that OAuth2 schemes utilize scopes with Swagger development.  When defining your OAuth2 scheme in your API, you simultaneously define scopes  components:   securitySchemes:     oAuthSample:       type: oauth2       flows:         implicit:           authorizationUrl: https://api.example.com/oauth2/authorize           scopes:             read_pets: read pets in your account             write_pets: modify pets in your account paths:   /pets/{petId}:     patch:       summary: Updates a pet in the store       security:          - oAuthSample: [write_pets]  From the above example, you can see we’ve defined two scopes which boil down to read and write access for users.  We’ve applied the “write” scope to the /pets/{petId} endpoint, indicating the use of this endpoint is reserved for those users with “write_pets” authorizations.  This is the power of scopes in Swagger API development; they let us lock down whole APIs or individual endpoints based on very customizable access settings.   Securing your Security  It is worth noting Basic, Bearer, and API Key Authentication are easily reversible or spoofable.  By inspecting packets, you can easily hijack API Keys or Bearer Tokens.  With Basic Authentication, you can even reverse engineer usernames and password.  These schemes should always be employed over an encrypted channel like HTTPS.  By using SSL/TLS encryption, your request headers, cookies, and query strings will be encrypted, thus keeping your authorization keys secure when using the API.   Deploying your Secure API  When you have finishing locking down your API endpoints using the described authorization methods, you can very easily generate stubbed server methods.  For example, I added Basic Authentication to one of the methods in the API example from previous posts on this topic and exported the stubbed server in Spring.  The YAML for that GET method was  paths:   /exercises:     summary: Returns a paginated list of exercises     description: Returns a paginated list of exercises     get:       tags:         - Exercise       security:        - BasicAuth: []     summary: 'Sample GET operation on the /exercises endpoint.'       description: 'This operation, defined by the noun \"exercises\" and the verb \"GET\" implies we will be making an HTTP GET request against the API, will receive multiple Exercise objects in the response.'     operationId: 'getAllExercises'     responses:         '200':             description: Successfully retrieved paginated list of all Exercise objects             content:                 application/json:                 schema:                     type: array                     items:                         $ref: '#/components/schemas/Exercise'         '401':             $ref: '#/components/responses/UnauthorizedError'         '500':             description: Server error prevented successful retrieval  The resulting stubbed method generated by Swagger for me was  @ApiOperation(value = \"Sample GET operation on the /exercises endpoint.\", nickname = \"getAllExercises\", notes = \"This operation, defined by the noun \\\"exercises\\\" and the verb \\\"GET\\\" implies we will be making an HTTP GET request against the API, will receive multiple Exercise objects in the response.\", response = Exercise.class, responseContainer = \"List\", authorizations = { @Authorization(value = \"BasicAuth\")    }, tags={ \"Exercise\", }) @ApiResponses(value = {      @ApiResponse(code = 200, message = \"Successfully retrieved paginated list of all Exercise objects\", response = Exercise.class, responseContainer = \"List\"),     @ApiResponse(code = 401, message = \"Authentication information is missing or invalid\"),     @ApiResponse(code = 500, message = \"Server error prevented successful retrieval\") }) @RequestMapping(value = \"/exercises\", produces = { \"application/json\" }, method = RequestMethod.GET) ResponseEntity&lt;List&lt;Exercise&gt;&gt; getAllExercises();  Why is this useful?  Well, as an architect designing an API to be written by an application developer, it may not be instantly clear which methods will be secured and how.  But when authoring APIs using Swagger Hub and generating stubbed server code in this way, it is nearly impossible to introduce developer errors into your code base.  Each stubbed method is fully annotated; the functionality is explicitly defined in a way that is easily recognized by architects and developers alike.     Furthermore, as an application developer looking to utilize this API endpoint, it is easy to see from the documentation page how to utilize this method.  By clicking on the “lock” icon next to the method I am trying to use, I can quickly see how I need to write my application to utilize this API endpoint.        Summary  Swagger Hub is a powerful and concise tool used to author explicit and well-defined APIs.  When used properly, Swagger can bridge the gap between architects, back-end developers, and front-end developers by creating an organic, dynamic representation of your API should be used.  Furthermore, the code-generation tools used by Swagger allow it to generate code agnostic APIs that fit your business use case and your developer’s skill sets.  Furthermore, Swagger’s strong adherence to the Open-API Specification 3.0 allows your API to be well-understood across all industries.  No matter how amazing a product is, if it is difficult to use, no one will use it.  Tools like Swagger help keep APIs easy to understand, easy to use, easy to document, and easy to develop against.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/using-swagger-to-build-effective-apis-part-3",
        "teaser": null
      },{
        "title": "Best Practices for Amazon EMR",
        "excerpt":"This blog post summarizes the main points in the AWS white paper titled “Best Practices for Amazon EMR” written by Parviz Deyhim in August 2013.  The paper can be found here and I encourage anyone working with EMR in the long-term to read it, as it is full of easy to understand concepts which will improve your use of EMR.  Some of the technical details are a little dated though, so anything very detailed should be double checked.  Those details are beyond the scope of this post; readers of this post should be looking to refresh their memories at a high level the best strategies for using EMR.   Main Takeaways     S3 is the recommended option over HDFS much of the time   Use Transient Clusters for your workloads to save on cost   Amazon EMR team has spent a lot of time tuning default settings, so don’t tweak default settings unless you really know what you’re doing and why you need to do it.     Moving Data Into EMRS      EMRS is best populated from S3      When populated EMRS from an existing cluster for the first time, it is best to use S3 as an escrow data storage location for your on-prem HDFS/non-HDFS data.      Use S3DistCp to copy data to S3      S3DistCp is the same as the Hadoop binary DistCp, except it takes advantage of multi-part upload to S3 for larger files.  Hadoop is optimized for large file blocks, so it is usually best to use S3DistCp for copying HDFS files from an external data center or local disk to S3 to take advantage of this optimization.      S3DistCp is faster than DistCp      The exception to this may come in very specific instances, where you need to specify the number of mappers needed for your copy job.  DistCp allows you to specify the number of mappers using CLI flags, whereas S3DistCp does not.  It is possible DistCp could perform faster in some cases when the proper number of mappers need to be specified manually.  However, the default formula used by DistCp is usually the best option for most workloads, which means S3DistCp is your best bet for copying data.       AWS Import/Export or Storage Volume Gateway to copy non-HDFS data into S3      Recommended AWS supported option for transferring data from a non-HDFS source to S3 at the time of publication.  At the time of this post’s writing, you could likely replace Import/Export with a Storage Gateway Volume which writes directly to S3.      Copy from S3 to EMR      Use the S3DistCp command to copy the data from your S3 bucket onto a Core Node.  Run the command on a core node, specifying the HDFS directory as the destination.  Let EMR handle the redundant copies to the other core nodes in the cluster.  You cannot run this on a Task or Master node as those nodes do not have HDFS on them.      EMRFS &gt; HDFS      Use EMRFS over HDFS to take advantage of S3 as a storage layer.  This works well when continuously adding data to your EMR cluster.  It is better to forward logs to S3 than directly to HDFS.  But when your cluster uses EMRFS, it gets the updates to the file system from S3.  This allows you to take advantage of bucket policies, bucket encryption, and object versioning for your EMR files.      Consistent View      For more information on how S3 bypasses eventual consistency for new PUT requests, check out Consistent View, a special feature of EMR which utilizes DynamoDB to bypass the eveentual consistency model employed by S3.      Iterative Jobs Should Use HDFS Only      Do not use S3 and EMRFS for iterative jobs, as you will introduce additional costs and latency for all of the repeated S3 GETs.      Aggregating Data for MapReduce      Fewer Large Files &gt; Many Small Files      This scheme takes advantage of multi-part uploads to S3, and reduces the number of connections required to upload data.  Additionally, fewer files stored in S3 improves performance for EMR reads on S3.      Log Forwarding      Where possible use a log forwarding framework like Apache Flume or Fluentd and write directly to the S3 bucket.      Aggregate Logs Based on Size      If your log aggregator can aggregate logs based on size, try to split your log files into large 1GB or 2GB chuncks.  This will be most performant for EMR.  If you cannot aggregate on size, aggregate on time taking into consideration log volume over time.      Recall the “Map” in MapReduce      This stage breaks large files into chunks for parallel processing.  That is why it is best to upload large files to HDFS or S3 for EMRFS.        Compress Your Files Before Processing      This will save on data transfer costs.  Recall though, EMR splits up files phase.  Make sure you consider your compression algorithm with your log aggregator.  Some compression schemes like GZip and Snappy will not allow the resulting compressed file to be split.  In that case, try not to surpass 1GB per compressed file.  If you do choose a “splitable” compression algorithm like LZO or BZip2, you can drastically increase the size of your compressed files up to 4GB.  Choose intelligently based on required compression speeds and ratios.  These factors are more important than choosing an aggregated file size.  You will hit a bottleneck in compression speed and ratios before you take a performance hit on S3 for logging more files.       Consider compressing Mapper Outputs      EMR allows you to compress the output of the “Map” function, during the “Reduce” function.  This is something to consider to save on data transfer costs.  You can enable this in the core node properties.      Consider compressing Mapper Outputs in Memory      EMR allows you to compress the memory footprint of the “Map” function as well.  This is an important consideration for large jobs that need to be completed quickly.  If there is a large amount of data to map, compressing the output of the mappers in memory will prevent the output from being written to disk.  You can enable this in the core node properties.       Tuning EMR      Instance Size      Use M2 instances for jobs requiring lots of memory.  C1 or C2 instances provide larger compute resources.  Consider spinning up clusters repeatedly until you discover the most performant instances for your workload.  Keep in mind, larger memory pools prevent disk writes while larger CPU pools process jobs faster.      Mapper Process Count      The best way to calculate this is to rely on the EMR defaults.  If you run a job on a default EMR cluster, you can see in the logs the number of mappers launched to process the job.  Divide this value with the number of Core and Task nodes you have.  The result is the number of mappers per EC2 instance.  From there, you can determine how many mappers per EC2 you will need, and can then make a decision on the instance sizes you will create in your cluster.       Transient Clusters      If the number of processesing hours on your cluster is less than a day, spin up a transient cluster.  The cluster will be terminated after the job is done, and you will save on costs.  Additionally, consider transient clusters when using EMRFS, as the data will be backed up in S3.  When the cluster dies, all cluster data will be backed up to S3, allowing you to resume processing at a later date from where you left off.  Generally, if you’re using S3 for storage, you will benefit from a transient cluster.      Cost Optimization      Use Spot Instances for Task Nodes      Task nodes offer additional compute, nothing else.  Consider Spot instances for Task nodes to take advantage of EC2 market pricing without introducing potential data loss on your cluster should the Spot instance price rise above what you are willing to pay.      Purchase a Reserved Instance for Heavy Utilized Nodes      The master node and the minimum number of Core nodes should be reserved instances.  This guarantees you will always have a node available to orchestrate your cluster as the master node.  Additionally, reserved instances for the Core nodes ensures your cluster can always perform at a minimum high efficiency.      Using Reserved Instances requires deep workload knowledge      Only purchase Reserved Instances after you know what your cluster’s workload will look like.  Only purchase reserved instances to cover the predictable, steady-state workload of the cluster.  Spot or on-demand instances should act as a buffer for workloads exceeding expectations.       Design Patterns      S3 &amp; EMRFS &gt; HDFS      This is pushed by AWS as the best option for most workloads.      S3 &amp; HDFS      Store data on S3 and copy to HDFS.  Introduces start up latency, but is preferred for iterative workloads to save on S3 GET requests.      HDFS with S3 Backup      Use the HDFS on the core nodes, and backup to S3 whenever needed.      Manual Tuning      Use EMR console and CloudWatch to tune and configure EMR Cluster to fit your use case.       Automatic Tuning      Use CloudWatch alarms to tune EMR in response to your cluster’s performance monitoring.  This image comes from the white paper.  The image suggests using Elastic Beanstalk as an intermediary between SNS and the EMR API.  The Beanstalk instance must be configured to interpret SNS messages generated by EMR CloudWatch alarms into EMR API commands.  Given the date of publication of this white paper, Beanstalk may not be the best service to use in this case any more.  I would suggest using API Gateway with Lambda functions to convert the SNS messages into EMR API calls; but the specific use case is relevant to the individual workload.      Summary of Themes       S3 with EMRFS and Consistent Views works great on Transient Clusters.   Compress your files intelligently based on required compression speeds and ratios.  Adjust your aggregated file size to reflect the splittability of the compression algorithm chosen.  If you cannot split the compressed files, ensure your files are no larger than 1GB.   Do not use S3 for workloads which consistently read against the log data   Purchase Reserved Instances to cover baseline, predictable workloads.  Purchase Spot instances for Task nodes, but not Core and Master nodes as the immediate removal of a Core or Master node could compromise the cluster’s health.   Partition your data intelligently, based on how you will use it.  If your log data is time sensitive, partition the logs based on time.  Make sure your files are on the larger side to take advantage of EMRFS/HDFS file processing.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/aws-white-paper-in-5-minutes-or-less-best-practices-for-emr",
        "teaser": null
      },{
        "title": "Ippon's RVA WAR Tour",
        "excerpt":"Ippon Technologies recently trained their two North America based Solutions Architects to conduct Well-Architected Reviews.  I am one of those architects, and I recently finished a week of conducting reviews on some of our best clients that are using AWS in some capacity.  This blog post is an account of that week and the lessons I learned.   What is a Well-Architected Review?  The Well-Architected Framework is a compilation of what AWS calls “tribal knowledge” which is focused on designing “well-architected” applications.  An application which is well-architected is, according to this framework, likely to perform very well under most circumstances as there have not been any identified risks within the applications architecture at that time.  If an application is well-architected, it adheres strongly to the principles and concepts outlined in the 5 pillars of the well-architected framework.  As you dig into the 5 Pillars, it becomes obvious how straightforward the well-architected status is.  The concepts covered in the well-architected review are common sense principles that identify resilient, efficient, and cost effective applications.   Why Would a Client Be Interested?  AWS has built a very competitive sales program around the Well-Architected Framework.  This framework creates an engine for AWS Certified Solutions Architects to look at cloud workflows and aid in the process of identifying potential risks in the deployed workload.  These workloads could be on paper as a diagram, or they could have been deployed in a cloud environment for 10 years now; the exact deployment state does not matter.  The point of a Well-Architected Review (WAR) is to fix problems in workloads before they cost your business.   The reason partner organizations, like Ippon, conduct these reviews is because we are a company which specializes in cloud technology.  Our consultants are certified AWS Solutions Architects, Developers, and DevOps Engineers; it is our job to stay current in the rapidly evolving cloud services space.  It is certainly possible to review your workload without our services, but unless your organization employs dedicated AWS Solutions Architects you will not have the benefit of an outside opinion with cloud deployment expertise on your workload.   Additionally, by conducting a WAR on a mission-critical cloud workload, your AWS Account becomes eligible for a $5,000 service credit towards any remediation work which is flagged up by a first time review.  This incentive allows your company to pay for any critical remediation items which may have been identified by the WAR.   Key Take-Aways from a Week of Reviews  I work in the New York Ippon office, but all of the reviews I conducted this week were based out of our Richmond delivery center.  I had the opportunity to meet face to face with clients whose workloads I had never been exposed to before.  As a result, I had been exposed to a vast range of workloads, configurations, and architectures in a very short period of time.  These are some of the key take-aways I recommend other Solutions Architects keep in mind when conducting their own reviews.     Cloud makes your architecture cleaner, no matter the size.   Early adopters will always have an uphill battle.   Cutting cost is one of the easiest remediation items to implement.   Cloud is Clean  I had the opportunity this past week to conduct a WAR for a small AWS client.  The organization had about 30 - 40 people employed, and they provided a service isolated primarily to the eastern seaboard, though they are growing fast.  I was not surprised to see their workload on AWS was simple, efficient, and effective.  The workload was scalable and very easy to troubleshoot.  It was designed very well and frankly it fit the bill for what I expected of a smaller sized company.  A few days later, I visited one of our global clients.  On the way to the review, I had expected to wade through layers and layers of architectural diagrams, finding arrows pointing to decommissioned services, and all other manner of architectural nuance characterized by large organizations with a global presence.  I was pleasently surprised to find I was incorrect here.  The large, multi-million dollar global corporation had a very clean, efficient workload in AWS which was easy to understand, troubleshoot and review.  So where was the problem?  Why did I feel like something wasn’t adding up?   It turns out, the problem was between my ears.  I had a pre-conceived notion about what factors create architectural complexity.  In one of my past lives, I used to work for a large bank as a Site-Reliability Engineer for several data centers.  I vividly remember looking at application flows and network diagrams, trying desperately to solve a production issue only to discover the architecture did not reflect the documentation.  I attributed this to the size of the company I was working with at the time.  This company was a global organization with thousands of servers in a single data center; dozens of data centers around the world.  There was so much to manage in the data center, it became impossible to keep our architecture risk free at all times, let alone keep the documentation up to date.  So if size didn’t determine architectural complexity, what did?   The big factor which influences architectural complexity is the location of your architecture.  If your compute and storage are located in a data center, there is too much to manage from an operations standpoint to hope to keep your architecture clean and concise.  However, once you move to the cloud, it becomes very easy to seperate your applications logically.  This separation drastically reduces operations expenses and creates a clean environment for you application stack.  A clean environment is easier to troubleshoot, support, and scale.   Uphill Battle  If you’ve been called “Cloud Evangelist” at your organization, it’s very likely there is a negative sentiment to cloud-based hosting.  As the “Cloud Evangelist,” you are one of a few employees that believes the cloud is the answer to your organization’s infrastructure problem.  Every new operations issue which arises confirms your belief in a cloud hosted future.  Every time you pay your data center fees, you grow more and more bitter that this very preventable expense is still an expense for your company.  Cloud Evanglists are usually the first people to promote a cloud migration in their company, and they usually experience the most resistance.   In my experience this week, it became very obvious to me that many clients simply are not ready to adopt the cloud.  There are many reasons for this, I can only speculate specifics on a client-by-client basis; but I can surmise a few root causes.   The first is fear of the unknown.  Organizations that view IT as a cost-center do not see the potential for IT to improve their business.  In those situations, you often have management and executive level employees that would rather maintain the status quo in the technology department.  This stability frees them to focus on their business.  The irony is, by ignoring IT and the incessant ramblings of the Cloud Evangelist, the executive is losing out on the chance to improve their IT department.   The second is fear of lost ROI.  By adopting a cloud provider, you are admitting on some level that your data center costs are too high.  The cost of buying servers, buying network equipment, paying for rack space, and paying for the operational overhead is so astronimcally high, you would think most people would jump at the chance to cut costs.  But when you are a business that prides itself on lasting business partnerships, it may be difficult to justify terminating a contract for something you may not be convinced is worthwhile, especially if you have been paying millions of dollars a year to maintain those business partnerships.   This past week made it obvious to me that companies always have a reason to not adopt a cloud based data center solution.  That is why the early adopter has the hardest time in their organization.  It takes a big rudder to stear a big ship.  A small rudder can stear a big ship too, but it will take a lot more time.   How Many Pennies on the Dollar?  There is no question, cloud services are cheaper than on-premises data centers.  The cost is literally pennies on the dollar for many services.  This feature of the cloud is what drives adoption.  Going into the reviews this week, I expected to see bills on par with mortgage payments.  Some clients even got their bills so low, they resembled credit card payments!  However, no matter how impressive the monthly AWS bill was, every client had the same question.  “How much am I paying within my organization, and I can I bring that cost down?”   As your organization grows to use AWS services, your monthly bill can quickly become obfuscated by curious engineers spinning up and spinning down resources.  Some engineers probably even forget to spin down those resources at the end of the day, thus increasing the bill.  AWS offers powerful accelerated deployment services via push-to-deploy mechanisms which exist across many of their services.  The unfortunate drawback to this is your bill can grow faster than you realize without a clear reason why.   The solution to this problem is simple, but it requires a dilligent architect to see it through.  By tagging your resources, you can generate reports in the Cost and &amp; Usage Explorer which are filtered by tags.  This allows you to discover the costs associated with all of your QA resource for example.  If you have resources which contribute to your monthly charge tagged “QA,” you can easily develop a report which shows all of these resources and how much they cost you that month.  From there, it’s very easy to isolate (and therefore shame) the developer that leaves all of their EC2 instances on over the weekend!  Check out this link on AWS recommended tagging strategies.  A good tagging strategy will empower you to make more cost effective decisions regarding resource allocation.   Going Forward  The reviews Ippon conducted this week taught me a lot about the Well-Architected Framework.  I’m very excited to take these lessons learned into future client reviews; we want to offer the best advice possible to all of our clients.  It is my belief these takeaways will help us, and any other Solutions Architect, to conduct helpful reviews which improve cloud workloads for all of our clients.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/ippons-rva-war-tour",
        "teaser": null
      },{
        "title": "Java 13 Release Features",
        "excerpt":"Ever since Oracle took over the long-term advancement of the Java programming language, Java has been upgraded on a semi-annual release schedule.  Tuesday, September 17th marks the official release date of Java 13.  This release is not LTS (Long-Term Support); many of its features lay the groundwork for larger transitions in the languages long-term development, as seen between versions 8 and 12.  Additionally, it is worth mentioning this release falls under the OpenJDK.   Useful Links     JDK 13 JEP Summary   JDK 13 Download Link   JDK 13 API Document   Comparing Java LTS Releases   Feature Summary  Let’s take a closer look at some of the features added to Java 13.  These features are broken out by JEP number (JDK Enhancement Proposals).  The JEP indexing system is similar to the more established RFC protocal used industry-wide to define well-established definitions like TCP (as well as some less known memos).  The JEP index defines the roadmap for future Java releases, though it is worth mentioning not every JEP makes it into a release (JEP 352, postponed to Java 14).   We’ll start this article with some of the sexier additions to Java in this release, and finish up with features more interesting to the die-hard Java developers out there.  JEP 354 - Switch Expressions (Preview)  JEP 354 is my personal favorite because it is another leap forward in the quest to make Java more functional, and less procedural.  Traditionally, switch statements perform a sequence of operations based on the value held by the variable you are “switching” on.  Using the case keyword, developers can execute statements that are dependant on the value of the so-called “switch”.   This paradigm has always possessed a few crucial flaws.  The first being the required break statement at the end of each case. Without the break statement, a logical switch becomes a sequence of statements.  For example,  String foo = \"foo\"; switch(foo) {     case \"foo\":         foo = \"I'm a foo.\"     case \"bar\":         foo = \"I'm a bar.\" } System.out.println(foo);  will not print out “I’m a foo.”  Instead, it prints out “I’m a bar.” because there was no break statement halting the execution from going forward.  For large switch blocks, the extra and unnecessary break statements can quickly bloat your code.   The other crucial flaw with traditional switch statements is scope is shared across every case statement in the block.  This means any variable declared in one case block exists in another case block.  String foo = \"foo\"; boolean semaphore; switch(foo) {     case \"foo\":         boolean result = true;         semaphore = result;     case \"bar\":         System.out.println(result);         boolean result = false;         semaphore = result;  In this example, the first case block is executed.  The variable result is initialized, and it is used to assign a value to a boolean outside the switch’s scope.  Then, the second case block is executed.  Technically, an error would be thrown because we are trying to create a variable result when one already exists.  But if the error was not thrown, the application would print “true” because the value of result is passed down from the first case statement.  To get around this, you would have to create a variable result2.  This creates additional unnecessary lines of code and those extra, hardly used variables create more overhead for the garbage collector.   In Java 13, the new switch statement does away with the break statement, and it creates an individual scope for each case clause.  It also introduces the yield keyword, which is a great way to return values from switch clauses.  The yield keyword also allows switch blocks to be used declaratively as statements.  Take the above example, if we were to rewrite it in Java 13, we would have the following:  String foo = \"foo\"; boolean semaphore = switch(foo) {     case \"foo\" -&gt; yield true;     case \"bar\", \"foobar\" -&gt; yield false;     default -&gt; yield true; }; System.out.println(semaphore);  The output of this snippet is “true,” thanks to the yield keyword.  Without using any break statements and without creating additional variables, we’ve assigned a conditional value to another variable using syntax similar to a lambda or anonymous class, without actually creating a lambda or anonymous class.  This new design of the switch statement makes it a valuable tool for making readable code which is easy to extend and modify.   Some long-time Java developers may be hesitant to use a new keyword like yield, and for good reason.  The yield keyword is a hair’s width away from the return keyword in terms of functionality.  They perform very similar tasks, the main difference is when you would use them.  To truly elucidate the reason for the yield keyword, let’s rearrange the above example slightly:  String foo = \"foo\"; boolean semaphore = switch(foo) {     case \"foo\" -&gt; true;     case \"bar\" -&gt; false;     default -&gt; {         int sample = 100;         yield true;     } };  The default case in the above example shows the situation where there are multiple steps to complete in this case.  To accommodate the passing of a value from the individual cases context back to the super-context of the switch, we must yield that value.  In the cases of \"foo\" and \"bar\" this step is unnecessary, as there is only one possible value to be yielded up to the super-context.  JEP 355 - Text Blocks (Preview)  This feature of Java 13 is more cosmetic than any other feature in this release.  Effectively, it replaces line-breaks in Java via the + sign with simple carriage returns.  To utilize text blocks, you must initialize a string with three quotation mark symbols.  This makes text blocks like  String text = \"This is a former example\\n\" +                 \" of multi-line text in\\n\" +                 \" pre-Java 13 times.\"  capable of being re-written like this:  String text = \"\"\"                 This is a modern text block.                 There is no need for the newline character at all.                 We just add a new line and it is processed automatically.                 No more crazy escape sequences! \"\"\";  This syntax is great for writing application code in other languages like Scala and passing it to an interpreter object within your application.  Another great use case is embedding formatted HTML in your Java application.  The rules for text blocks are just like string literals.  You can concatenate text blocks with String literals, you can insert variables into String literals, you can even insert the single quote “ into text blocks without having to escape it; even escape sequences are supported if you wish to add them in!  JEP 353 - Reimplement the Legacy Socket API  JEP 353 covers the new implementation of the Socket API.  To be more precise, an additional, more modern implementation of the Socket API (which was introduced in Java 1), has been added to the core libraries.  The old Socket API is available via a JDK system property and the PlainSocketImpl class (but not for long).   The first, and in my opinion most important feature of the new implementation, is that sockets are polled in non-blocking mode and are accessed via an operation limited by a timeout by default.  Why is this important?  Effectively, it means you can perform operations on a Socket object without having to wait for the Socket to respond before making an additional operation on said Socket.  Consider an application that relies on external APIs for a subset of its functionality.  If you start the application with a blocking Socket, you will have to wait for your Socket operations to complete before continuing your application.  But if you were to initiate a connection to an API service using a non-blocking socket and store the result in a Future&lt;&gt; object, you can continue your application’s initialization steps without waiting for a response.  This is particularly convenient when decoupling your application’s initialization and third-party dependencies.  JEP 350 - Dynamic CDS Archives  This particular enhancement modifies the JRE more than anything; most Java developers will not notice this enhancement in their day to day development.  The origins of this JEP start in Java 10, with JEP 310.  Class-Data Sharing has been around since JDK 5, but it was codified as “Application Class-Data Sharing” for Java 10.  Effectively what happens is meta-data across a developer-specified list of classes are shared in an archive file.  This archive file is then loaded and referenced by the JVM.  If this archive file is built from 100 class files, you save the JVM a lot of time and energy by referencing just the one file.  This reduces memory footprint and application load times.   So what feature has been added to CDS in Java 13?  Java 13 introduces dynamic archiving.  Before Java 13, making a CDS archive required the completion of several steps.  As outlined in JEP 350, the steps are:     Do one or more trial runs to create a class list   Dump an archive using the created class list   Run with the archive These steps are all achievable via java CLI options.  Depending on the size of the application, these commands may be difficult to run.  This is where Dynamic CDS comes in.  This enhancement performs step 1 at the end of your application’s execution, let’s say in your QA build process.  From here its just a matter of building your archive file and dropping it into a CI/CD pipeline for deployment.  The manual steps required to build the class list required to create the archive is completely gone with Dynamic CDS Archives.     JEP 351 - ZGC: Uncommit Unused Memory      ZGC was a new garbage collector introduced to Java in Java 11.  It was designed to work on environments with massive compute capacity and memory requirements, significantly greater than desktop computing.  While other garbage collectors have their perks, ZGC is the best choice for applications with significant memory and compute requirements.       ZGC is not without limits though.  Before Java 13, ZGC never returned memory to the OS without a restart.  This feature is common to most garbage collectors; most traditional GCs return memory to the OS by default as they were designed to run on commodity or even embedded hardware with severely restricted memory and compute requirements.  Not being able to return memory to the OS would have significant repercussions on application performance.  If we consider the origins of ZGC though, it is easy to see why this feature was not built-in.  If you have thousands of gigabytes of RAM installed on your host machine, you likely do not need to return it to the OS.  Regardless, this feature was added to ZGC in Java 13, making it an option for applications that do not always run on specialized, enterprise-level machines.  Future Directions  Java 13 is the next chapter in the quest to make Java a viable functional language.  Since Oracle has taken over the development of Java, it has gradually evolved into a modern language, capable of keeping up with the hottest trends in application development.  Since Java 8, the language has steadily evolved into a discrete, easily readable, easily extensible language capable of keeping up with other languages like Python, Scala, and Kotlin.   At the time of this post’s writing, Java 14 is scheduled to be released in March of 2020.  JEP 352 is the only addition scheduled for Java 14, but that last will likely grow to include several more JEPs from the index before the end of 2019.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/java-13-release-features",
        "teaser": null
      },{
        "title": "AWS Well-Architected 2020",
        "excerpt":"Ippon USA has been working to lock in Well-Architected status in the North America region for the last six months.  By the end of 2019, Ippon USA will join Ippon France in meeting the Well-Architected program requirements for two consecutive quarters.  Over the last 6 months, the WAR (Well-Architected Review) team at Ippon has learned through experience.   This past week, I was lucky enough to attend re:Invent 2019 where I learned other AWS Partners have had similar experiences.  In addition to the new announcements surrounding Well-Architected, there’s a lot to think about for how Ippon will use the program in 2020.   Well Architected is a Mindset  The 5 pillars of Well Architected are often called “AWS Tribal Knowledge.”  But the principles that define these pillars are not proprietary AWS concepts.  The strategies and techniques used to build performant, reliable, and secure applications which are easy to maintain and inexpensive have been well established for decades.  So why is the Well-Architected program so valuable to clients?  Based on what I learned at re:Invent 2019, it seems like Well Architected is not valuable to clients unless they truly practice Well-Architected at every level of their organization.   The most successful AWS Partners in the Well-Architected (WA) space ensure all of their employees, including sales, are well-versed in the WA Framework.  This company wide adoption allowed one of the speakers at re:Invent to conduct over 200 WA reviews in 2019, with a rate of rollover into additional business.  A similar company touted the same number of reviews, but expressed difficulty in converting reviews into additional business.  This is not to say the architects at Company A are better at reviewing than the architects at Company B.  The big difference between the two organizations is how they treated the 5 pillars of WA internally.  Company A said they dove into WA head-first, while Company B implied they were testing the program out.  This hesitance is evidenced by the niche group of WA focused employees at Company B, compared to the front-to-back adoption of WA at Company A.   When your entire organization adopts WA principles, these common sense tenants that any seasoned engineer would claim to practice on a daily basis anyway, the organization will retain new business as a consequence.   Well-Architected is a Sales Tool  The WA framework is a great addition to a sales pitch for new clients.  Ippon Technologies’s motto is “Discovery to Delivery.”  This usually means initial engagements with a client start with a “discovery” period where we interview engineers and product owners, define deliverables, build a project timeline and crank out as many relevant demos as possible.  After this process, which can be lengthy depending on the proposed scope, we hit the ground on “delivery,” usually in an agile project management style.   Before re:Invent, we never rolled WA reviews into the “discover” phase of our engagements.  Now, we will start introducing WA reviews as part of the initial assessment.  By conducting a review early on, it becomes clear to all parties exactly what the rest of the assessment needs to focus on.  The scope of the engagement is well-defined within the confines of the review process.  In addition to raising awareness for the technical flaws which need to be remedied during the engagement, the WA review also raises awareness for larger organizational issues that could be addressed during the engagement.   For example, if our client is looking to add a new feature to their app before they sign contracts for new business in a different region, the Operational Excellence, Performance, and Reliability pillars should be used to assess if the client is ready to scale their business as they have planned.  Armed with this knowledge, Ippon engineers can be ready to scope DR replication, CDS deployments, and even a hiring process for new engineers on behalf of the client.  Just by allocating three or four hours at the beginning of the “discovery” phase to have a focused discussion on WA concepts, we will be able to streamline “discovery” initiatives and deliver a stronger product by the end of the engagement.   Well-Architected is not an Innovation Driver  After re:Invent 2019, I am so eager to use all of the shiny new features that can be found on AWS.  Some of this features can be quite expensive though, and can introduce complexity into an otherwise simple problem.  This creates solutions which are not Well-Architected.  My last, and most important, note on WA at Ippon in 2020 is that Well-Architected is not going to be an excuse to introduce new, innovative tools and services for the sake of innovating.   One of the presenters at the WA event shared story after story of how when they started their journey with the WA program, the temptation to recommend cutting edge tools and services on AWS was too great to resist.  This created technical debt in the recommendations that rolled into an operational cost for their engineering staff.  I am happy to say, the partner quickly remedied this practice internally.  Now, the remediations they suggest include innovative tools and services when and where they are required.   It may be difficult to assess what could be considered “over-engineering” when conducting a review.  After all, if your job is review and improve architectures, it may not be over-engineering to you to recommend AWS Managed Kafka as a Service to introduce a message bus to decouple applications when simple SQS will work just fine.  With that in mind, the company suggested getting to know technical staff as part of the review.  By having an understanding of how the engineering staff likes to work, whether it’s open source over managed solutions, Windows over Linux, Kubernetes or plain ECS; this information helps to shape recommendations.   Need a workload reviewed?  If you like what you read, reach out!  We’d love to conduct a review of one of your workloads.  Check out our Amazon Partner Network page to begin scheduling a WAR.  Or, reach out to me directly at dferguson@ipponusa.com and I’ll work with you to schedule your first review with our team of highly skilled reviewers.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/aws-well-architected-2020",
        "teaser": null
      },{
        "title": "Creating an Event Driven jHipster Application with Solace",
        "excerpt":"Today, the modern web-application is event driven.  In order to be an event-driven application, you need two things: the event, and message bus or mesh on which to send the event.  Typically Kafka is the first and last platform used by developers to achieve event-driven architectures, primarily because it has always been free and is relatively easy to setup.  However, Kafka, especially Confluent Kafka, is better suited as an event database due to its built in message log and replay capabilities.  But Kafka presents some difficulties in a production environment.   Maintaining a Kafka infrastructure requires very careful consideration and planning, especially if event ordering matters to your application.  For organizations that cannot afford a messaging operations team to maintain their message bus, either financially or operationally, an event driven architecture can be more of a nightmare than synchronous API calls.  This is where Solace shines.   Solace is a message bus that got its start in the finance industry.  Originally, the product offering was restricted to hardware accelerated routers that would publish and subscribe small messages very quickly over a networks with varying performance in order.  To achieve this, the engineers at Solace built routers containing specialized NICs with a custom TCP stack built on them designed for extremely low latency messaging.  Because of the specialized nature of these routers, they were very expensive and only saw use in enterprise companies requiring ultra low-latency messaging like hedgefunds and banks.  With the advent of virtualization technologies and the maturation of cloud providers, its easy and inexpensive to deploy Solace routers anywhere in the world!   Why is this so important?  As development paradigms shift towards event-driven architecture, it is becoming more crucial now more than ever to have reliable, scalable, low-cost, and operationally efficient message backbones to facilitate your applications event footprint.  But just how difficult is it to develop an event-driven application using Solace as a message bus?  Let’s try it out!  In this post, I will show you how to integrate jHipster with Solace to develop an event-driven jHipster web application.   Provision a Solace Router  First, navigate &amp; login to the Solace Cloud Console.  Follow the prompts to create a “service,” be sure to specify free tier and AWS as the cloud provider.  Other cloud providers are offered, but being that AWS owns the majority of the market share and Ippon is an AWS Advanced Consulting Partner, we’re going to use AWS for this demo.  Select a region which supports the free tier of Solace in the Cloud (hint: US-East N.Virginia works).  Then, give a good name to your service.  Your screen should look like this:Click the “Start Service” button and wait until your service is provisioned.   Once your service is started, you should be redirected to a dashboard with tabs for “Status” and “Manage.” I won’t go into details regarding these tabs as that goes beyond the scope of this post.  Instead, navigate to the “Try Me!” tab.  Here you can see the publisher configuration is on the left side, while the subscriber configuration is on the right.  By clicking “Connect” on both sides, you will connect to the broker twice: once as a publisher and once as a subscriber.  Now you can follow the prompts to publish and subscribe to messages in this window.  Experiment with the topic settings in the “Topic” input box.  Notice how publishing a message on a topic you are not subscribing to will not show the message in the subscriber window.   Solace Connections FYI  Solace connections are just TCP connections.  When you click the “Connect” button in the “Try Me” tab, you are creating a TCP connection using the SMF protocal.  SMF is a proprietary message protocol developed by the engineers at Solace.   Solace Topics FYI  Topics are a well-established concept in messaging, and in the Solace world this is no different.  However, Solace imparts more structure around their topics than exists in other open source messaging solutions like RabbitMQ or Kafka.  In Solace, topics have levels seperated by the “/” character.  This allows you to partition your topics bery precisely along the levels specified.  For publishers, this is important as it allows the publisher to publish a very specific topic.   This is also very important for subscribers.  Subscribers can subscribe to very specific events, or to a large set of wild-carded events.  For example, if the publisher sends a message on the topic USER/CREATE/&lt;ID&gt; where the ID field is the ID of the user created, the subscriber can subscribe to messages on the topic USER/CREATE/&gt;.  This topic represents all “create user” events pertaining to any user.  If we wanted all messages for all users, we could subscribe to USER/*/&gt;, or more succinctly USER/&gt;.  The &gt; character is a wildcard representing all unspecified levels of a topic beyond the specified level.  The * character is a wildcard representing any string of characters at that level.   Build your App  The Solace Cloud Console may be interesting for a little while, but it’s not hard to see how, by itself, this isn’t very useful.  How can you publish and subscribe programmatically within an application?  In this demo, we’ll start with a basic jHipster project.  The details of your application are irrelevant, but I would recommend starting with a Monolithic style application for the purposes of this demo.  While your jHipster project is being generated, navigate to the “Connect” tab in your Solace Cloud Console.  Select the “Solace Java API” drop-down and copy the appropriate API import statement which will changed if you use Maven or Gradle.   After your project is generated, open the build.gradle file and paste the API reference into the appropriate location.  At the time of this writing, the current API reference is compile(\"com.solacesystems:sol-jcsmp:10.2.0\").  If you’re using Maven, drop the XML formatted reference into the appropriate section of your pom.xml file.  Be sure to run the ./gradlew command to download this library into your project.  Once this is done, click “Next” in the Solace Cloud Console.    The full repository is here on Github (see bottom for full URL) for your personal enjoyment.  However, for brevity’s sake, I’ll outline the major code changes below.  Assuming you’ve modified the build.gradle file, create this service file:  package com.ippon.dferguson.service;  import com.solacesystems.jcsmp.*; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Transactional;  @Service @Transactional public class SolacePublisherService {      final JCSMPProperties properties = new JCSMPProperties();     private JCSMPSession session = null;      public SolacePublisherService() throws JCSMPException {         properties.setProperty(JCSMPProperties.HOST, \"\");         properties.setProperty(JCSMPProperties.USERNAME, \"\");         properties.setProperty(JCSMPProperties.PASSWORD, \"\");         properties.setProperty(JCSMPProperties.VPN_NAME, \"\");     }      public void connect() throws JCSMPException {         session = JCSMPFactory.onlyInstance().createSession(properties);         session.connect();     }      boolean isOpen(){         return !session.isClosed();     }      public void disconnect() {         if(session != null &amp;&amp; !session.isClosed()) {             session.closeSession();             session = null;         }     }      public void publish(String payload, String publish_topic) throws JCSMPException {         XMLMessageProducer prod = session.getMessageProducer(new JCSMPStreamingPublishEventHandler() {             @Override             public void responseReceived(String messageID) {                 System.out.println(\"Producer received response for msg: \" + messageID);             }             @Override             public void handleError(String messageID, JCSMPException e, long timestamp) {                 System.out.printf(\"Producer received error for msg: %s@%s - %s%n\",                     messageID,timestamp,e);             }         });         final Topic topic = JCSMPFactory.onlyInstance().createTopic(publish_topic);         TextMessage msg = JCSMPFactory.onlyInstance().createMessage(TextMessage.class);         msg.setText(payload);         prod.send(msg,topic);         prod.close();     } }  Most of the code here can be found in the publish/subscribe Solace tutorial.  The SolacePublisherService properties have been redacted for security reasons, but the connection details are auto-generated by the Solace Cloud Console and populated by the connection code snippets.   This class file creates a Solace publisher service.  We don’t care about a subscriber at this time.  In this simplistic event driven application, we can assume the user is creating events and does not care about events being created at all.  In other words, the user is a publisher, publishing events via the application.  The application does not need a subscriber component at this time.  If it did though, the process for integrating a subscriber in your application would be very similar to integrating a publisher.   Now that we have the service built and wired into the spring context built by jHipster, we need to tell our application to publish an event.  Open the UserService file and add this snippet to the bottom of the createUser method:  try {     solacePublisherService.connect();     solacePublisherService.publish(\"WELCOME TO THE PLATFORM\", \"USER/CREATE/\" + userDTO.getId());     solacePublisherService.disconnect(); } catch(JCSMPException e){     log.error(\"Could not publish the User Create message!\", e); }  Modify the constructor of the UserService file accordingly:  private final SolacePublisherService solacePublisherService;  public UserService(UserRepository userRepository, PasswordEncoder passwordEncoder, AuthorityRepository authorityRepository, CacheManager cacheManager, SolacePublisherService solacePublisherService) {     this.userRepository = userRepository;     this.passwordEncoder = passwordEncoder;     this.authorityRepository = authorityRepository;     this.cacheManager = cacheManager;     this.solacePublisherService = solacePublisherService; }   Test It Out  Now that we have our application code modified, let’s build and test our application using the Solace Cloud Console.   Start by returning to the “Try Me!” tab in the Solace Cloud Console.  Connect to your Solace router as a subscriber and subscribe to the topic USER/CREATE/&gt;.   Build the jHipster application as the Readme instructs: run ./gradlew and npm start.  Once your jHipster application is built, log into the application as the admin (default username and password is admin/admin).  Then, navigate to the User Administration tab in the top-right corner and select the “User Management” button.  Select the “Create New User” button in the top-right corner, fill in the blanks, and click the button to create the user.   Once the user is created in your app, return to the Solace Cloud Console’s “Try Me!” tab.  You should see a message pop up in the Solace Cloud Console similar to the following:  Congratulations!  You’ve just sent an event over a message bus in response to an application event driven by a user’s behavior!   This demo is a very simplistic example of developing an event-driven application.  The published message is not very helpful and the application only publishes one type of message.  To truly take advantage of an event-driven architecture, you could for example subscribe to a creation event and send a welcome e-mail to the new user.  Or if your application has an e-commerce component, you could publish and subscribe page views to help your data science team determine which products pique consumer interest.  The applications are virtually unlimited.  The only important aspect of event-driven application design are having atomic, consistent, independent and durable events for application components which are important to your business.   External Links  Github Repo - https://github.com/dferguson992/solace-jhipster-demo  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/creating-an-event-driven-jhipster-application-with-solace",
        "teaser": null
      },{
        "title": "Building a HIPAA Compliant Message Bus with Solace",
        "excerpt":"What is Solace?  Solace are the makers of the Solace PubSub+ Platform which is used to event enable your enterprise. The core of the platform is the Solace PubSub+ Advanced Event Broker that comes in hardware, software, and messaging as a service in your favorite public cloud.   Cloud-Native Solace  In the last few years, Solace has begun investing heavily into cloud based messaging solutions. Using tools like the Solace Cloud Console, you can quickly provision redundant and secure virtual solace appliances in any cloud you choose. But Solace messaging isn’t just fast, it is reliable, extensible, and secure. Solace virtual routers can seamlessly be deployed as an HA pair in a single region and DR can be setup across regions to ensure availability constraints that your enterprise demands. Solace routers support nearly every major messaging protocol, the list of protocols not supported is shorter than list of protocols that are. Solace routers are also secure. Administrative access for a private cloud deployment is as secure as the EC2 instances in your Amazon console.  All of these built-in features, coupled with native cloud deployments with a very generous free-tier, make Solace an obvious option for businesses outside the financial space.  Companies spend thousands of dollars and person-hours building reliable messaging deployments that are secure and cost-effective.  Now you can have those features at the click of a button thanks to the cloud.  (Check out my last article talking about exactly how easy it is to do this.)  But not every business vertical is convinced cloud-based solutions like Solace fit their needs.  One of those  business verticals is healthcare and insurance.  The real question for these verticals is around HIPAA.  In this article we’ll discuss what HIPAA is and why Solace is absolutely a viable choice for messaging in a HIPAA compliant environment.  By taking the time to properly configure your PubSub+ services, achieving HIPAA compliance with Solace is quite straightforward. I’ll show you how to get started in this article.   What is HIPAA?  HIPAA, or Health Insurance Portability and Accountability Act, is a piece of legislation passed in 1996 that puts regulation around the security of Personal Health Information (PHI).  Health insurance organizations deal with PHI all the time, this is the very nature of an insurance claim.  In the 24 years since this legislation was made law, the digital world has changed drastically, but compliance requirements remain the same.  For a full list of HIPAA compliance requirements, check out this website.  The list of requirements are extensive, but they do boil down to the following short-list of absolute necessities.     Implement a means of access control. This not only means assigning a centrally-controlled unique username and PIN code for each user, but also establishing procedures to govern the release or disclosure of ePHI during an emergency.   Introduce activity logs and audit controls. The audit controls required under the technical safeguards are there to register attempted access to ePHI and record what is done with that data once it has been accessed   Enforce policies for the use/positioning of workstations.  Policies must be devised and implemented to restrict the use of workstations that have access to ePHI, to specify the protective surrounding of a workstation and govern how functions are to be performed on the workstations.   Enforce policies and procedures for mobile devices. If users are allowed to access ePHI from their mobile devices, policies must be devised and implemented to govern how ePHI is removed from the devices if the user leaves the organization or the device is re-used, sold, etc.   Conduct regular risk assessments. Among the Security Officer´s main tasks is the compilation of a risk assessment to identify every area in which ePHI is being used, and to determine all of the ways in which breaches of ePHI could occur.   Introducing a risk management policy. The risk assessment must be repeated at regular intervals with measures introduced to reduce the risks to an appropriate level. A sanctions policy for employees who fail to comply with HIPAA regulations must also be introduced.   Develop a contingency plan. In the event of an emergency, a contingency plan must be ready to enable the continuation of critical business processes while protecting the integrity of ePHI while an organization operates in emergency mode.   Restrict third-party access. It is vital to ensure ePHI is not accessed by unauthorized parent organizations and subcontractors, and that Business Associate Agreements are signed with business partners who will have access to ePHI.   Assessment Assumptions  This article is about defining a HIPAA compliant Solace implementation for companies dealing with PHI.  Let’s define what a typical environment would look like for a company like this.     User login to applications is handled by a central authentication/authorization platform like LDAP or Active Directory.   Applications all have their own usernames for the various integrations they possess, but end user activity is still logged and tracked for audit and control purposes.   Datastores, third party integrations, and application integrations have a application-user level permissions.  A typical example would be grants to select data from a table in a database for a specific application.   The company bound to HIPAA compliance is presumed to have a secure network from within the physical premises of the company.  Remote logins are secured through IPSec, VPNs, or some other secure communication protocol.   Applications used by the organization are already HIPAA compliant.   The organization already has a HIPAA compliant cloud-adoption policy in place that honors the phyical requirements of a HIPAA compliant workspace.   The organization already has a HIPAA Risk Assessment &amp; Management Policy in place that can be applied to new services like PubSub+.   This list of 5 assumptions about an organization trying to be HIPAA compliant is fairly general.  Federated user login, application activity logging, intelligent access controls around data, and secure networking are all standard requirements for any business that makes money from data.  PHI data and Insure-tech companies are no exception.  Given the above mentioned general settings for an organization, we will discuss how the required aspects of HIPAA can be honored in a Solace Cloud deployment.     TLS / encryption to secure PHI/PII data in transit)   Encrypted storage to secure PHI/PII data at rest   Fine-grained access controls to govern and ensure only authorized systems can publish data and can receive data   Auditing of changes to security configurations   Technical Safeguards  Implement a Means of Access Control  Access control in Solace is a built-in, out of the box component of Solace.  Access control on Solace are baked in down to the underlying operating system.  For cloud deployments however, the true HIPAA compliant access control comes from User Authentication/Authorization, ACLs, and Client-Profiles. Authenticating against a Solace appliance can be done a number of ways.  The simplest is with a username and password.  This is the default setting for free-tier deployments.  There are a number of other built-in authentication solutions used by Solace that are quite extensible; an entire series of blogs could be written on this topic alone.  Concerning HIPAA compliance, Solace integrates easily with LDAP.  By authorizing against LDAP, your Solace access controls have the same HIPAA compliance as your LDAP configuration.  Configuring LDAP to be HIPAA compliant is a well-established and well-documented process; it suffices to say that if your LDAP configuration is HIPAA compliant, your Solace configuration is HIPAA compliant. In addition to LDAP integration, Solace has some built-in configuration options that can supply fine-grained access control down to the topic level.  Like most messaging systems, Solace has Access-Control Lists.  ACLs in Solace define the topics a user can publish and subscribe to, without the user having to know ahead of time.    If the user attempts to pub/sub against a topic not on their ACL, an error will be thrown.  Additionally, Solace ACLs can enforce IP whitelists or blacklists.  This means a user is only allowed to pub/sub messages from a very specific network location.  Typically, this is a physical location in an office or the end of an encrypted VPN tunnel.  This kind of control is not required by HIPAA, but it is a recommended step to take when trying to become HIPAA compliant. Another great feature of Solace that provides fine-grained, HIPAA compliant access control are client profiles.  Like the various methods of authentication to Solace, client profiles are an extensive topic in Solace configuration that require an in-depth knowledge of low-latency networking to fully grasp.  Typically, these settings are worth modifying as a workaround for upstream networking issues.  However, it’s worth noting some of the less specific features that could be configured in a HIPAA compliant environment.  When configuring a client profile, it is important to remember that clients using this profile will be restricted to the actions and settings in that profile.  There are some client specific settings that can configured, but general usability is restricted at the profile level.  For example, if you never want a client to connect to a queue, you would not allow them to receive guaranteed messages.  If you wanted to specify a service level user that could only create bridged connections (connections between different Solace Message VPNs or Services), you would enable “Connect as a Bridge.”  If you only want to publish messages when there is an active subscriber receiving them, you would enable the “Reject Messages to Sender on No Subscription Match Discard” setting.  These are all standard, high-level settings that define the behavior for a set of clients using your Solace device.  HIPAA compliance comes in with the “Downgrade Connection to Plain Text” setting.  This is a setting you would immediately disable when configuring Solace.  Under no circumstances should you downgrade the encryption on a connection in a HIPAA compliant environment.  By disabling this setting, all users on that client profile will automatically follow this setting.   Introduce Activity Logs and Audit Controls  If you want to view logs on a Solace router, simply subscribe to the topic #LOG/&gt;.  This does require a small amount of configuration on the router side (see below), but once enabled, your configured log messages will be routed to the appropriate topic for consumption.   Instead of writing to log files, Solace will track activity by publishing messages to the internal logging topic #LOG.  By setting up a queue or an always-on consumer bound to the #LOG/&gt; topic, you will always have up to date logs for your app.  Furthermore, log messages are broken out by topic hierarchy.  This allows you to have very granular logging configurations across all of your consumers.  In short, Solace’s logging capabilities extends to meet your organization’s HIPAA requirements; you just may have to think outside of the box about acquiring those logs.  You no longer need to setup an FTP site or run a tail -F logs/*.  If you think about logs as messages, it becomes very easy to acquire logs in a scalable manner to achieve HIPAA compliance.   Administrative Safeguards  Developing a Contingency Plan  Solace PubSub+ appliances come with the pre-configured option of establishing highly available routing, with the option to add a disaster recovery node.  This triplet of appliances ensures always-on performance with no data loss in case of a disaster.  Restricting Third-Party Access  The rules governing third-party access to a PubSub+ service are as straightforward as restricting access to corporate e-mail.  Secure passwords and proper network configuration make PubSub+ services quite secure from third-party access.  This is true for any cloud deployment.  Consider AWS’s VPC construct.  By default, it secures resources access through security groups, route tables and network access control lists.  When deploying a Solace PubSub+ appliance inside a corporate VPC, your PubSub+ appliance is bound to the same access restrictions which protect all of your assets from third-party access.   Final Thoughts on HIPAA Compliance with Solace  Solace is a HIPAA compliant messaging solution.  Solace appliances have always had this capability, even before the company released a cloud-native deployment of their messaging technology.  By taking the time to properly configure your PubSub+ services, achieving HIPAA compliance is quite straightforward.  Here’s a short-list of recommendations to get your organization started.  From here, use your organization’s specific requirements to ensure PubSub+ services will assist you in all your messaging needs.      Define a logging consumer that writes #LOG/&gt; messages to Splunk, or an Elasticsearch cluster.   Deploy your PubSub+ in an AWS VPC within a private subnet.  Allow routing between a primary and secondary appliance, with fail-over traffic going to the DR appliance in a different region.   Deploy a PubSub+ image or Virtual Machine into your private cloud and manage it completely independent of Solace and the Cloud Console.  (AWS has an AMI on the Marketplace that allows you to deploy a Solace PubSub+ appliance using the EC2 service for example.)   Impart failover best-practices into your environment.  In a triplicate deployment, the requirements are a little tricky.  Check out this document from Solace that defines best practices for failover in a PubSub+ environment.   Establish a white-list of subnets that are permitted to connect to the appliances.  From there, determine which clients belong to which subnets and allocate topics they are permitted to publish and subscribe to using ACLs.   Apply a security layer to the Solace APIs that meets your organization’s security standards.  For example, this layer could publish all messages using your organization’s encryption policies.   For more information on Solace’s PubSub+ service, or if you have a specific question, check out https://solace.community/.  This website is constantly monitored by Solace professionals looking to answer any question you may have.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/posts/hipaa-compliant-message-bus",
        "teaser": null
      }]
